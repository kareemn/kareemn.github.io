<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Kareem Nassar by kimocode</title>
    <style>
       .video-container { position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden; }
       .video-container iframe, .video-container object, .video-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
    </style>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Kareem Nassar</h1>
        <p>Coder and Observer</p>


        <p class="view">
			<a href="https://github.com/kareemn">GitHub</a>
			<br/>
			<a href="http://angel.co/knassar">AngelList</a>
			<br/>
			<a href="http://www.linkedin.com/in/kareemnassar">Linkedin</a>
			<br/>
		</p>

      </header>
      <section>
        <h3>Smartmiq Deskstop Assistant</h3>
        <p>In 2021, I built a voice desktop assistant called Smartmiq that allows users to trigger actions using their voice. Similar to Mac's Spotlight search, Smartmiq allows users to perform tasks such as searching Google or replying to Slack messages with just their voice.</p>
        <img src="images/smartmiq_search_google.gif"/>
        <p>Or you could use it to reply to slack messages:</p>
        <img src="images/smartmiq_slack_reply.gif"/>
	<p>One of the unique aspects of Smartmiq is that I developed my own automatic speech recognition (ASR) system using PyTorch and the transducer architecture. This allowed me to achieve a high level of customization and control over the voice recognition capabilities of the assistant.</p>
	<p>I also focused on creating a user interface for Smartmiq that closely resembled the look and feel of Spotlight, in order to make it familiar and intuitive for users. However, as I continued to use and develop Smartmiq, I began to realize that voice input on desktop and laptop computers has not gained as much traction as it has on mobile devices. This is likely due to the fact that typing is often faster and more convenient on these devices, especially in public settings.</p>
	<p>Despite this realization, I received positive feedback from users who found Smartmiq to be a useful and convenient tool. Overall, building Smartmiq was a valuable learning experience that helped me to better understand the potential and limitations of voice input on desktop and laptop computers.</p>

      </section>
      <section>
        <h3>Using Deep Reinforcement Learning to play Pong</h3>
        <p>I trained a Deep Reinforcement Learning model based on soft Actor-Critic using architecture similar to the <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">AtariNet</a> arch, Monte Carlo roll-out, experience replay, and importance sampling (since its off-policy). The "soft" part of the actor-critic maintains some entropy in the probability distribution of the actions to continue to encourage exploration. See <a href="https://arxiv.org/abs/1801.01290">this paper</a> for more details.
        </p>
        <p>After a few iterations, looks like it learned something:</p>
        <div class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/bMo1Gg-ymD8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </div>
        PyTorch training code can be found <a href="https://gist.github.com/kareemn/c760f652ffb62cb565f99f926d807025">here</a>
      </section>
      <section>
        <h3>Noise Suppression Neural Network</h3>
        <p>In 2020, I trained a neural network based on <a href="https://google.github.io/speaker-id/publications/VoiceFilter/">VoiceFilter</a> architecture. The idea was learn a mask to apply to spectrograms in order to mask out non-speech noise.  </p>
        <p>Here's what it sounds like:</p>
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/Mon3kjfvdx0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </section>
      <section>
        <h3>Rover Music Visualizer using Generative Adverserial Network</h3>
        <p>
          I was looking for an excuse to learn <a href="https://www.rust-lang.org/">Rust</a>. I decided to build a music visualizer using Rust and a <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adverserial Network</a>.
        </p>
        <p>
          The general idea is to use the frequency components of the streaming music to explore the <a href="https://en.wikipedia.org/wiki/Latent_space">latent space</a> of a <a href="https://github.com/robbiebarrat/art-DCGAN">pretrained art GAN</a>.
        </p>
        <p>
          For each timestep in the music:
          <ul>
            <li>compute frequency components</li>
            <li>generate a vector based on frequency components and previous timestep's vector (we don't want the art to change too quickly).</li>
            <li>pass the vector to the GAN to produce the art</li>
            <li>render art in frame buffer</li>
          </ul>
        </p>
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/a9yJmaswq2o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        Code found here: <a href="https://github.com/kareemn/rover">https://github.com/kareemn/rover</a>
      </section>

      <section>
        <h3>Voice Assistant for your Zoom/Google Meet/Webex meetings</h3>
        <p>In 2018, I built a voice assistant that could join your Zoom meetings. I emulated the webcam and audio drivers in a docker container with a headless chrome browser. Our team at Workfit/Voicera/Voicea (we changed the name a few times...) eventually scaled it up on a kubernetes cluster.</p> 
        <p>We built our own wakeword detector and (eventually) our own speech recognition engines.</p>
        <p>Here's a video of what it looked like in a Zoom meeting:</p>
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/1FbBXDxFG7g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </section>
      <!-- <section>
        <h3>My first neural network using TensorFlow</h3>
        In 2016, I took an online course called <a href="https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-iv">Creative Applications of Deep Learning with TensorFlow.</a>
        For this class's second project, I am attempting to train a neural network using <a href="http://www.tensorflow.org">TensorFlow</a> to draw this:</p>
        <img src="images/first_nn/starry_night.png"/>

        <p>Here is final output of the neural network. Not bad for a 600-neuron brain.</p>
        <img src="images/first_nn/final.png"/>
        <br/><br/>
        Here is a visualization of the neural network's output during training. The graph shows the cost over time for each batch interation.
        <img src="images/first_nn/gradient_descent.gif"/>


      </section> -->

      <section>
        <h3>Poynt Nay-Nay</h3>
        <p>
          In 2015, I worked on the OS of a <a href="https://www.poynt.com/">payment terminal</a>. We built a <a href="https://support.poynt.com/hc/en-us/articles/360057832733-Introduction-to-Mission-Control">remote management feature</a> for pushing out OS and app updates.
          In order to test it out, we pushed a music video to all of our test devices simultaneously.
        </p>
        <p>
          The following is what ensued:
        </p>
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/7pg6mZ9HXU4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </section>
      <footer>
        <p><small>Hello &mdash; there</small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-41313960-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
