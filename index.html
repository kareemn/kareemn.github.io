<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Kareem Nassar by kimocode</title>
    <style>
       .video-container { position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden; }
       .video-container iframe, .video-container object, .video-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
    </style>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Kareem Nassar</h1>
        <p>Coder and Observer</p>


        <p class="view">
			<a href="https://github.com/kareemn">GitHub</a>
			<br/>
			<a href="http://angel.co/knassar">AngelList</a>
			<br/>
			<a href="http://www.linkedin.com/in/kareemnassar">Linkedin</a>
			<br/>
		</p>

      </header>
      <section>
        <h3>Smartmiq Deskstop Assistant</h3>
        <p>In 2021, I built a voice desktop assistant called Smartmiq that allows users to trigger actions using their voice. Similar to Mac's Spotlight search, Smartmiq allows users to perform tasks such as searching Google or replying to Slack messages with just their voice.</p>
        <img src="images/smartmiq_search_google.gif"/>
        <p>Or you could use it to reply to slack messages:</p>
        <img src="images/smartmiq_slack_reply.gif"/>
	<p>One of the unique aspects of Smartmiq is that I developed my own automatic speech recognition (ASR) system using PyTorch and the transducer architecture. This allowed me to achieve a high level of customization and control over the voice recognition capabilities of the assistant.</p>
	<p>I also focused on creating a user interface for Smartmiq that closely resembled the look and feel of Spotlight, in order to make it familiar and intuitive for users. However, as I continued to use and develop Smartmiq, I began to realize that voice input on desktop and laptop computers has not gained as much traction as it has on mobile devices. This is likely due to the fact that typing is often faster and more convenient on these devices, especially in public settings.</p>
	<p>Despite this realization, I received positive feedback from users who found Smartmiq to be a useful and convenient tool. Overall, building Smartmiq was a valuable learning experience that helped me to better understand the potential and limitations of voice input on desktop and laptop computers.</p>

      </section>
      <section>
        <h3>Using Deep Reinforcement Learning to play Pong</h3>
	
	<p>I trained a deep reinforcement learning model to play the classic game of Pong using the soft actor-critic algorithm. The soft actor-critic algorithm is a variant of the actor-critic algorithm that uses a "soft" version of the action distribution, in order to encourage exploration during training.</p>
<p>To process the game's image frames and predict the appropriate actions, I implemented a series of convolutional neural networks (CNNs) using PyTorch. Similar to the <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">AtariNet</a> arch, CNNs consisted of four convolutional layers. These layers were followed by a fully-connected layer with 128 units, and two output layers, one for the action value function (the critic) and one for the policy (the actor).</p>
<p>To train the model, I used the Adam optimizer and a mean squared error loss function for the critic, and a negative log likelihood loss function for the actor. I also employed techniques such as Monte Carlo roll-out, experience replay, and importance sampling to improve the model's performance.</p>
<p>Monte Carlo roll-out involves simulating a complete episode of the game, starting from the current state, and using the current policy to choose actions until the episode is completed. The rewards obtained during the episode are then used to update the policy. This technique helps to improve the model's ability to plan ahead and make more informed decisions.</p>
<p>Experience replay involves storing a dataset of past experiences (i.e., state-action-reward-next state tuples) and sampling from this dataset when updating the policy. This helps to decorrelate the experiences and stabilize the learning process.</p>
<p>Importance sampling is a technique used to correct for the fact that the data distribution changes over time due to the learning process. It involves weighting the importance of each experience based on how similar it is to the current policy. This helps to correct for the fact that the model may be learning from experiences that are no longer representative of the current policy.</p>
	      <p>After several iterations, my model was able to learn how to play Pong and achieved reasonable performance. You can see a demonstration of the model in action in this YouTube video:</p>
        <div class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/bMo1Gg-ymD8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </div>
        <p>The PyTorch training code for this project can be found <a href="https://gist.github.com/kareemn/c760f652ffb62cb565f99f926d807025">here</a> </p>
	<p>Overall, training this deep reinforcement learning model was a challenging but rewarding experience that taught me about the capabilities and limitations of this type of algorithm.</p>
      </section>
      <section>
        <h3>Noise Suppression Neural Network</h3>
        <p>In 2020, I trained a neural network based on <a href="https://google.github.io/speaker-id/publications/VoiceFilter/">VoiceFilter</a> architecture. The idea was learn a mask to apply to spectrograms in order to mask out non-speech noise.  </p>
        <p>Here's what it sounds like:</p>
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/Mon3kjfvdx0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </section>
      <section>
        <h3>Rover Music Visualizer using Generative Adverserial Network</h3>
        <p>
          I was looking for an excuse to learn <a href="https://www.rust-lang.org/">Rust</a>. I decided to build a music visualizer using Rust and a <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adverserial Network</a>.
        </p>
        <p>
          The general idea is to use the frequency components of the streaming music to explore the <a href="https://en.wikipedia.org/wiki/Latent_space">latent space</a> of a <a href="https://github.com/robbiebarrat/art-DCGAN">pretrained art GAN</a>.
        </p>
        <p>
          For each timestep in the music:
          <ul>
            <li>compute frequency components</li>
            <li>generate a vector based on frequency components and previous timestep's vector (we don't want the art to change too quickly).</li>
            <li>pass the vector to the GAN to produce the art</li>
            <li>render art in frame buffer</li>
          </ul>
        </p>
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/a9yJmaswq2o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        Code found here: <a href="https://github.com/kareemn/rover">https://github.com/kareemn/rover</a>
      </section>

      <section>
        <h3>Voice Assistant for your Zoom/Google Meet/Webex meetings</h3>
        <p>In 2018, I built a voice assistant that could join your Zoom meetings. I emulated the webcam and audio drivers in a docker container with a headless chrome browser. Our team at Workfit/Voicera/Voicea (we changed the name a few times...) eventually scaled it up on a kubernetes cluster.</p> 
        <p>We built our own wakeword detector and (eventually) our own speech recognition engines.</p>
        <p>Here's a video of what it looked like in a Zoom meeting:</p>
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/1FbBXDxFG7g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </section>
      <!-- <section>
        <h3>My first neural network using TensorFlow</h3>
        In 2016, I took an online course called <a href="https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-iv">Creative Applications of Deep Learning with TensorFlow.</a>
        For this class's second project, I am attempting to train a neural network using <a href="http://www.tensorflow.org">TensorFlow</a> to draw this:</p>
        <img src="images/first_nn/starry_night.png"/>

        <p>Here is final output of the neural network. Not bad for a 600-neuron brain.</p>
        <img src="images/first_nn/final.png"/>
        <br/><br/>
        Here is a visualization of the neural network's output during training. The graph shows the cost over time for each batch interation.
        <img src="images/first_nn/gradient_descent.gif"/>


      </section> -->

      <section>
        <h3>Poynt Nay-Nay</h3>
        <p>
          In 2015, I worked on the OS of a <a href="https://www.poynt.com/">payment terminal</a>. We built a <a href="https://support.poynt.com/hc/en-us/articles/360057832733-Introduction-to-Mission-Control">remote management feature</a> for pushing out OS and app updates.
          In order to test it out, we pushed a music video to all of our test devices simultaneously.
        </p>
        <p>
          The following is what ensued:
        </p>
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/7pg6mZ9HXU4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </section>
      <footer>
        <p><small>Hello &mdash; there</small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-41313960-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
